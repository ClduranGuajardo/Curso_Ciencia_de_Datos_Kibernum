{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a235c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-01-24T14:19:05.146764-03:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.7\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 158 Stepping 11, GenuineIntel\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895a911b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>descripcion</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aunque parezca mentira, las emisiones de dióxi...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hubo un proyecto impulsado por la Unión Europe...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China ha confirmado la conclusión con éxito de...</td>\n",
       "      <td>tecnología</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En su fructífera carrera como humorista, actor...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tras dos años de negociación entre la instituc...</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         descripcion   categoria\n",
       "0  Aunque parezca mentira, las emisiones de dióxi...     cultura\n",
       "1  Hubo un proyecto impulsado por la Unión Europe...     cultura\n",
       "2  China ha confirmado la conclusión con éxito de...  tecnología\n",
       "3  En su fructífera carrera como humorista, actor...     cultura\n",
       "4  Tras dos años de negociación entre la instituc...     cultura"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "noticias = pd.read_csv(\"noticias.csv\")\n",
    "noticias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a6fdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16495, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a27f275c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cultura       9001\n",
       "tecnología    4198\n",
       "ocio          3296\n",
       "Name: categoria, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticias.categoria.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4805474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea90d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7e4dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stopwords-iso/stopwords-es\n",
    "# import nltk from nltk.corpus import stopwords sw_nltk = stopwords.words('spanish') print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84893a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"stopwords-es.json\") as fname:\n",
    "    stopword_es = json.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d6fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_es[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f35c76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizador = TfidfVectorizer(strip_accents=\"unicode\",stop_words=stopword_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d71c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "                            '_', 'a', 'actualmente', 'acuerdo', 'adelante',\n",
       "                            'ademas', 'ademÃ¡s', 'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                            'ahi', 'ahora', 'ahÃ\\xad', 'al', 'algo', 'alguna',\n",
       "                            'algunas', 'alguno', 'algunos', 'algÃºn', ...],\n",
       "                strip_accents='unicode')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2299f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "ms = vectorizador.fit_transform(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf6be4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16495, 59952)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9721d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse == dispersa\n",
    "print(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95db657",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend\n",
    "# rasbt.github.io/mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0fd157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from mlxtend.preprocessing import DenseTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b285ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae45f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_gauss = make_pipeline(vectorizador,DenseTransformer(),GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a1b6339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6',\n",
       "                                             '7', '8', '9', '_', 'a',\n",
       "                                             'actualmente', 'acuerdo',\n",
       "                                             'adelante', 'ademas', 'ademÃ¡s',\n",
       "                                             'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                                             'ahi', 'ahora', 'ahÃ\\xad', 'al',\n",
       "                                             'algo', 'alguna', 'algunas',\n",
       "                                             'alguno', 'algunos', 'algÃºn', ...],\n",
       "                                 strip_accents='unicode')),\n",
       "                ('densetransformer', DenseTransformer()),\n",
       "                ('gaussiannb', GaussianNB())])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec67f9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(stop_words=['0', '1', '2', '3', '4', '5', '6',\n",
       "                                             '7', '8', '9', '_', 'a',\n",
       "                                             'actualmente', 'acuerdo',\n",
       "                                             'adelante', 'ademas', 'ademÃ¡s',\n",
       "                                             'adrede', 'afirmÃ³', 'agregÃ³',\n",
       "                                             'ahi', 'ahora', 'ahÃ\\xad', 'al',\n",
       "                                             'algo', 'alguna', 'algunas',\n",
       "                                             'alguno', 'algunos', 'algÃºn', ...],\n",
       "                                 strip_accents='unicode')),\n",
       "                ('densetransformer', DenseTransformer()),\n",
       "                ('gaussiannb', GaussianNB())])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test =\n",
    "pipeline_gauss.fit(X = noticias.descripcion, y = noticias.categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fde150f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cultura', 'cultura', 'tecnología', ..., 'cultura', 'tecnología',\n",
       "       'ocio'], dtype='<U10')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_gauss.predict(noticias.descripcion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a389ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6adf66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_curso(modelo, X, y):\n",
    "    preds = modelo.predict(X)\n",
    "    return f1_score(y,preds, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98844437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.62988784, 0.64352834, 0.64807517, 0.63776902, 0.63504092])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_gauss, noticias.descripcion, noticias.categoria, scoring = f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1dd1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_bernoulli = make_pipeline(TfidfVectorizer(strip_accents=\"unicode\",stop_words=stopword_es,max_features=500)\n",
    "                               ,DenseTransformer()\n",
    "                               ,BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53f5282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.66595938, 0.66110943, 0.66414065, 0.66565626, 0.65807821])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_bernoulli,noticias.descripcion,noticias.categoria,scoring=f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e23c105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "pipeline_multinomial = make_pipeline(TfidfVectorizer(strip_accents=\"unicode\",stop_words=stopword_es,max_features=500)\n",
    "                               ,DenseTransformer()\n",
    "                               ,MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9c5d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "C:\\Users\\Gamer\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aa', 'aaon', 'adema', 'adia3', 'afirma3', 'agrega3', 'aha', 'ais', 'ala3', 'algaon', 'alla', 'amos', 'an', 'antaa', 'aoltima', 'aoltimas', 'aoltimo', 'aoltimos', 'aqua', 'as', 'asa', 'asegura3', 'bamos', 'ca3mo', 'comenta3', 'considera3', 'cua', 'da3nde', 'deja3', 'dema', 'despua', 'detra', 'estara', 'estuvia', 'explica3', 'expresa3', 'fua', 'haba', 'habra', 'hubia', 'indica3', 'informa3', 'is', 'lla', 'llas', 'llega3', 'llos', 'ma', 'manifesta3', 'menciona3', 'ndo', 'nes', 'ningaon', 'nta', 'ntas', 'nto', 'ntos', 'paa', 'podra', 'pra3ximo', 'pra3ximos', 'qua', 'queda3', 'quia', 'ramos', 'realiza3', 'sa', 'sa3lo', 'sas', 'segaon', 'semos', 'sos', 'sta', 'stas', 'ste', 'stos', 'tambia', 'tao', 'tena', 'tendra', 'todava', 'trava', 'tuvia'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.68778418, 0.67626554, 0.67262807, 0.68475296, 0.66717187])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline_multinomial,noticias.descripcion,noticias.categoria,scoring=f1_curso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa727f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
